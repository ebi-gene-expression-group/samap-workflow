configfile: "config.yaml"

wildcard_constraints:
    prefix1 = "[^\/\.]+",
    prefix2 = "[^\/\.]+",
    results_tag = "[^\/]+",

name1, name2 = [ x['id'] for x in config.get('data') ]

rule all:
    input:    
        map="%s/%s/%s_%s.celltype_map.tsv" % (config.get('outdir'), config.get("results_tag"), name1, name2),
        heatmap="%s/%s/%s_%s.celltype_map_heatmap.png" % (config.get('outdir'), config.get("results_tag"), name1, name2)

rule unzip_transcriptome:
    input:
        fagz=lambda wildcards: [ x['transcriptome'] for x in config['data'] if x['id'] == wildcards.prefix ][0]

    output:
        fa=temp('{prefix}.fa')

    shell:
        "zcat {input.fagz} > {output.fa}"


## Blast section 

# BLAST searches can be sped up significantly if spread over multiple compute
# nodes. The database is composed from the full set, but queries are divided,
# and the results concatenated after.

rule split_transcriptome: 
    conda:
         'envs/fasta-splitter.yml'
    input:
        fa="{prefix}.fa"
    
    output:
        fa=temp(expand("{{prefix}}.part-{n}.fa", n=range(1, config.get('blast').get('splits')+1)))

    params:
        splits=config.get('blast').get('splits')

    shell:
        "fasta-splitter --nopad --n-parts {params.splits} --out-dir $(pwd) {input.fa}"

rule make_blast_db:
    conda:
         'envs/blast.yml'
    
    input:
        fa="{prefix}"
 
    output:
        db=temp(expand('{{prefix}}.{ext}', ext = config.get('blast').get('db_exts')))

    params:
        type=config.get("blast").get("type")

    shell:
        "makeblastdb -in {input.fa} -dbtype {params.type}"

rule tblastx:
    conda: 'envs/blast.yml'
    
    threads: config.get('blast').get('threads')

    input:
        query="{prefix1}.part-{n}.fa",
        db=expand('{{prefix2}}.fa.{ext}', ext = config.get('blast').get('db_exts')),
        dbfa="{prefix2}.fa"

    output:
        map = temp("{dir}/{prefix1}_to_{prefix2}.{n}.txt")

    shell:
        "tblastx -query {input.query} -db {input.dbfa} -outfmt 6 -out {output.map} -num_threads {threads} -max_hsps 1 -evalue 1e-6"

rule merge_blast:
    input:
        maps = expand("{{dir}}/{{prefix1}}_to_{{prefix2}}.{n}.txt", n=range(1, config.get('blast').get('splits')+1))

    output:
        merged=protected("{dir}/{prefix1}_to_{prefix2}.txt")
    
    shell:
        "cat {input.maps} > {output.merged}"

## SAMap section

# Make a transcript/gene mapping from the transriptome (assuming ensembl-like
# gene ID strings in the fasta headers

rule transcript_to_gene:
    input:
        fa=lambda wildcards: [ x['transcriptome'] for x in config['data'] if x['id'] == wildcards.prefix ][0] 

    output:
        txt=temp("{path}/{prefix}.t2gene")

    shell:
        """
        zcat {input.fa} |  grep '>' | awk '{{print $1"\\t"$4}}' | sed 's/>//' | sed 's/gene://g' | sed -r 's/\.[0-9]+//g' > {output.txt}
        """

# Do the initial SAMap initialisation step. This takes the longest, so protect
# the output.

rule samap_samap:
    conda: 'envs/samap.yml'

    input:
        transcript_to_gene1="{path}/{prefix1}.t2gene",
        transcript_to_gene2="{path}/{prefix2}.t2gene",
        maps = lambda wildcards: [ "%s/maps/%s" % (wildcards.path, file) for file in [ '%s%s/%s_to_%s.txt' % (wildcards.prefix1, wildcards.prefix2, wildcards.prefix1, wildcards.prefix2), '%s%s/%s_to_%s.txt' % (wildcards.prefix1, wildcards.prefix2, wildcards.prefix2, wildcards.prefix1) ] ]

    output:
        pkl=protected("{path}/{results_tag}/{prefix1}_{prefix2}.sam.pkl")

    params:
        anndata1=lambda wildcards: [ x['anndata'] for x in config['data'] if x['id'] == wildcards.prefix1 ][0], 
        anndata2=lambda wildcards: [ x['anndata'] for x in config['data'] if x['id'] == wildcards.prefix2 ][0], 
        reset_to_raw='--reset-to-raw' if config.get('reset_to_raw') is not None and config.get('reset_to_raw') else ''

    resources:
        mem_mb=lambda wildcards, attempt: attempt * 32684

    shell:
        "samap-samap --id1={wildcards.prefix1} --id2={wildcards.prefix2} {params.reset_to_raw} {params.anndata1} {params.anndata2} {wildcards.path}/maps/ {input.transcript_to_gene1} {input.transcript_to_gene2} {output.pkl}"


# Note: memory usage for samap-run scales with number of threads.

rule samap_run:
    conda: 'envs/samap.yml'
    
    input:
        sam="{path}/{prefix1}_{prefix2}.sam.pkl"

    output:
        sam="{path}/{prefix1}_{prefix2}.run.sam.pkl",
        tsv="{path}/{prefix1}_{prefix2}.celltype_map.tsv",
        png="{path}/{prefix1}_{prefix2}.celltype_map_heatmap.png"
    
    resources:
        mem_mb=lambda wildcards, attempt, threads: attempt * threads * 10240
         
    threads: config.get('samap-run').get('threads')
    
    params:
        cell_type_field=config.get('cell_type_field'),       

    shell:
        "samap-run --celltype_field1 {params.cell_type_field} --celltype_field2 {params.cell_type_field} --ncpus {threads} {input.sam} {output.sam} {output.tsv} {output.png}"

